---
title: "CombinedClassifier"
author: "Anyan Liu"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{CombinedClassifier}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



## 1. Introduction

`CombiningClassifier` is an R function that combines multiple machine learning classifiers (logistic regression, decision tree, random forest, SVM, and XgBoost) to provide robust predictions. This document demonstrates its usage, functionality, and performance.


## 2. Preparing Data

The first step is to create a synthetic dataset for demonstration.
```{r data-preparation, echo=TRUE}
# Simulating training and test datasets
set.seed(123)
train_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100))
train_labels <- sample(c(0, 1), 100, replace = TRUE)
test_data <- data.frame(x1 = rnorm(20), x2 = rnorm(20))
```

## 3.Using Package
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

# Load the required package
library(CombiningClassifier)

# Run the classifier
result <- CombiningClassifier(train_data, train_labels, test_data)

# Display results
print(result)

```


## 4. Comparison efficiency
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(bench)
library(randomForest)

set.seed(123)
train_data <- data.frame(x1 = rnorm(100), x2 = rnorm(100))
train_labels <- sample(c(0, 1), 100, replace = TRUE)
test_data <- data.frame(x1 = rnorm(20), x2 = rnorm(20))

performance <- bench::mark(
  Combining = CombiningClassifier(train_data, train_labels, test_data),
  Logistic = {
    model <- glm(train_labels ~ ., data = train_data, family = binomial)
    as.numeric(predict(model, test_data, type = "response") > 0.5)
  },
  iterations = 10
)

print(performance)

```

## 5. Comaprison correctness
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)



```


## 6.Use testthat to do unit testing 
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(devtools)
devtools::test()

```


## 7.Key function code
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

CombiningClassifier <- function(train_data, train_labels, test_data) {

  if (!all(train_labels %in% c(0, 1))) {
    stop("train_labels must only contain 0 and 1")
  }

  if (!all(names(test_data) == names(train_data))) {
    stop("test_data columns must match train_data columns")
  }

  library(rpart)
  library(randomForest)
  library(e1071)
  library(class)
  library(xgboost)
  # Logistic Regression
  log_model <- glm(train_labels ~ ., data = train_data, family = binomial)

  # Decision Tree
  tree_model <- rpart(train_labels ~ ., data = train_data, method = "class")

  # Random Forest
  rf_model <- randomForest(train_data, train_labels, ntree = 300, mtry = 2)

  # SVM
  svm_model <- svm(train_labels ~ ., data = train_data, kernel = "radial", cost = 1, gamma = 0.1)

  # XGBoost
  xgb_train <- xgb.DMatrix(data = as.matrix(train_data), label = as.numeric(train_labels) - 1)
  xgb_model <- xgboost(data = xgb_train, max_depth = 3, nrounds = 100, objective = "binary:logistic", verbose = 0)

  # Logistic Regression
  log_pred <- ifelse(predict(log_model, test_data, type = "response") > 0.5, 1, 0)

  # Decision Tree
  tree_pred <- as.numeric(as.character(predict(tree_model, test_data, type = "class")))

  # Random Forest
  rf_pred <- ifelse(predict(rf_model, test_data, type = "response") > 0.5, 1, 0)

  # SVM
  svm_pred <- ifelse(predict(svm_model, test_data) > 0.5, 1, 0)

  # XGBoost
  xgb_pred <- ifelse(predict(xgb_model, as.matrix(test_data)) > 0.5, 1, 0)

  # integrete
  predictions <- data.frame(
    Logistic = log_pred,
    DecisionTree = tree_pred,
    RandomForest = rf_pred,
    SVM = svm_pred,
    XGBoost = xgb_pred,
    KNN = knn_pred
  )

  # Weighted vote
  weights <- c(0.2, 0.1, 0.4, 0.2, 0.4, 0.1)
  combined_pred <- apply(predictions, 1, function(row) {
    row <- row[row %in% c(0, 1)]

    if (length(row) == 0) {
      return(sample(c(0, 1), 1))
    }

    unique_vals <- unique(row)
    weighted_votes <- sapply(unique_vals, function(val) sum(weights[row == val]))
    unique_vals[which.max(weighted_votes)]
  })


  return(as.numeric(combined_pred))
}



```



Vignettes are long form documentation commonly included in packages. Because they are part of the distribution of the package, they need to be as compact as possible. The `html_vignette` output type provides a custom style sheet (and tweaks some options) to ensure that the resulting html is as small as possible. The `html_vignette` format:

- Never uses retina figures
- Has a smaller default figure size
- Uses a custom CSS stylesheet instead of the default Twitter Bootstrap style

## Vignette Info

Note the various macros within the `vignette` section of the metadata block above. These are required in order to instruct R how to build the vignette. Note that you should change the `title` field and the `\VignetteIndexEntry` to match the title of your vignette.

## Styles

The `html_vignette` template includes a basic CSS theme. To override this theme you can specify your own CSS in the document metadata as follows:

    output: 
      rmarkdown::html_vignette:
        css: mystyles.css

## Figures

The figure sizes have been customised so that you can easily put two images side-by-side. 

```{r, fig.show='hold'}
plot(1:10)
plot(10:1)
```

You can enable figure captions by `fig_caption: yes` in YAML:

    output:
      rmarkdown::html_vignette:
        fig_caption: yes

Then you can use the chunk option `fig.cap = "Your figure caption."` in **knitr**.

## More Examples

You can write math expressions, e.g. $Y = X\beta + \epsilon$, footnotes^[A footnote here.], and tables, e.g. using `knitr::kable()`.

```{r, echo=FALSE, results='asis'}
knitr::kable(head(mtcars, 10))
```

Also a quote using `>`:

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))
