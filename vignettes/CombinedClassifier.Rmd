---
title: "CombinedClassifier"
author: "Anyan Liu"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{CombinedClassifier}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## 1. Introduction

`CombiningClassifier` is an R function that implement Logistic function glm(). Because of my initial misunderstanding of the assignment problem, the function name may be ambiguous. But note that it is a function that implements a glm function. The `CombiningClassifier` function is a custom implementation of logistic regression using the iterative reweighted least squares (IRLS) method. This document demonstrates its purpose, usage, and comparison with R's built-in `glm()` function for logistic regression.

The function computes:
- Logistic regression coefficients.
- Predicted probabilities for input data.
- The number of iterations required to achieve convergence.

---
# Function Overview
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
CombiningClassifier<- function(X, y, max_iter = 100, tol = 1e-6) {

  X <- as.matrix(cbind(Intercept = 1, X))
  y <- as.vector(y)

  beta <- rep(0, ncol(X))

  for (i in 1:max_iter) {

    p <- 1 / (1 + exp(-X %*% beta))

    gradient <- t(X) %*% (y - p)
    W <- diag(as.vector(p * (1 - p)))
    hessian <- -t(X) %*% W %*% X

    beta_new <- beta - solve(hessian) %*% gradient

    if (max(abs(beta_new - beta)) < tol) {
      beta <- beta_new
      break
    }
    beta <- beta_new
  }

  list(
    coefficients = beta,
    fitted.values = as.vector(1 / (1 + exp(-X %*% beta))),
    iterations = i
  )
}
```


## Preparing Data

The first step is to create a synthetic dataset for demonstration.
Generate a Binary Classification Datasetï¼š
```{r data-preparation, echo=TRUE}

# Simulating training and test datasets
set.seed(123)  
X <- data.frame(
  x1 = rnorm(100),  
  x2 = rnorm(100)
)
y <- sample(c(0, 1), 100, replace = TRUE)  

```



##  Package Using Method
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(CombiningClassifier)
result <- CombiningClassifier(X, y)
print(result)

```



##  Comparison efficiency with glm()
The efficiency performance of CombiningClassifier can be compared to R's built-in glm() function:
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(bench)

set.seed(123)
X <- data.frame(x1 = rnorm(100), x2 = rnorm(100))
y <- sample(c(0, 1), 100, replace = TRUE)

performance_comparison <- bench::mark(
  my_logistic = CombiningClassifier(X, y)$fitted.values,
  glm = fitted(glm(y ~ ., data = X, family = binomial)),
  iterations = 10
)

print(performance_comparison)


```



##  Comaprison correctness with glm()
The correctness performance of CombiningClassifier can be compared to R's built-in glm() function:
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)


set.seed(123)
X <- data.frame(x1 = rnorm(100), x2 = rnorm(100))
y <- sample(c(0, 1), 100, replace = TRUE)


CombiningClassifier_result <- CombiningClassifier(X, y)


glm_result <- glm(y ~ ., data = X, family = binomial)


fitted_mylogistic <- CombiningClassifier_result$fitted.values
fitted_glm <- fitted(glm_result)

all.equal(unname(fitted_mylogistic),unname(fitted_glm))

```



## Use testthat to do unit testing of CombiningClassifier
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(devtools)
devtools::test()

```

## Example
set.seed(123)
library(CombiningClassifier)
X <- data.frame(x1 = rnorm(100), x2 = rnorm(100))
y <- sample(c(0, 1), 100, replace = TRUE)

result <- CombiningClassifier(X, y)

cat("Estimated Coefficients:\n")
print(result$coefficients)

cat("\nFitted Probabilities (first 10):\n")
print(head(result$fitted.values, 10))

cat("\nNumber of Iterations:\n")
print(result$iterations)


## Conclusion
The CombiningClassifier function is a simple and effective implementation of logistic regression. While it may not replace optimized built-in functions like glm(), it provides a transparent and customizable way to understand how logistic regression works under the hood. The performance and predictions are comparable to glm(), as shown in the benchmarks and visualizations.

For further enhancements:
1. Add error handling for edge cases.
2. Support regularization techniques like LASSO or Ridge.
